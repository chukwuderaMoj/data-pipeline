{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "### Paco, Dera, Natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Spin up docker instance based on YML file configuration\n",
    "    * Command: docker-compose up -d\n",
    "    * Output: \n",
    "        Starting pacomiguelagv_zookeeper_1 ... done\n",
    "        Starting pacomiguelagv_mids_1      ... done\n",
    "        Starting pacomiguelagv_presto_1    ... done\n",
    "        Starting pacomiguelagv_cloudera_1  ... done\n",
    "        Starting pacomiguelagv_spark_1     ... done\n",
    "        Starting pacomiguelagv_kafka_1     ... done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create Kafka topic \"events\", input description, and run game_api.py file \n",
    "    * Script: sh step1_spinup.sh\n",
    "    * Commands: \n",
    "        echo \"Creating topic events...\"\n",
    "        docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "        echo \"Topic events description:\"\n",
    "        docker-compose exec kafka kafka-topics --describe --topic events --zookeeper zookeeper:32181\n",
    "        echo \"Spinning up game game_api.py...\"\n",
    "        docker-compose exec mids env FLASK_APP= /w205/w205_project3_Aguirre_Flowers_Mojekwu/pacomiguelagv/game_api.py flask run --host 0.0.0.0\n",
    "    * Output:\n",
    "Creating topic events...\n",
    "Topic events description:\n",
    "Topic: events   PartitionCount: 1       ReplicationFactor: 1    Configs: \n",
    "        Topic: events   Partition: 0    Leader: 1       Replicas: 1     Isr: 1\n",
    "Spinning up game game_api.py...\n",
    " * Serving Flask app \"game_api\"\n",
    " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Launch a Kafka consumer for the topic \"events\"\n",
    "    * Script: step2_consumer.sh\n",
    "    * Commands: \n",
    "        echo \"Launching Kafka Consumer...\"\n",
    "        docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning\n",
    "    * Output: (none until messages are produced, see step 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Produce random events using Apache Bench every 3 seconds\n",
    "    * Script: step3_producemessages.sh\n",
    "    * Commands:\n",
    "        watch -n 3 ./benchevents.sh\n",
    "        (see benchevents.sh for details)\n",
    "    * Output: \n",
    "        This is ApacheBench, Version 2.3 <$Revision: 1706008 $>0\n",
    "        Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/\n",
    "        Licensed to The Apache Software Foundation, http://www.apache.org/\n",
    "        Benchmarking localhost (be patient).....done[\n",
    "        Server Software:        Werkzeug/0.14.1\n",
    "        Server Hostname:        localhost\n",
    "        Server Port:            5000\n",
    "        Document Path:          /purchase_a_sword?id=3\n",
    "        Document Length:        51 bytes\n",
    "        Concurrency Level:      1\n",
    "        Time taken for tests:   0.013 seconds\n",
    "        Complete requests:      1\n",
    "        Failed requests:        0\n",
    "        Total transferred:      206 bytes\n",
    "        HTML transferred:       51 bytes\n",
    "        Requests per second:    77.51 [#/sec] (mean))\n",
    "        Time per request:       12.901 [ms] (mean)\n",
    "        Time per request:       12.901 [ms] (mean, across all concurrent requests) \n",
    "    \n",
    "    * Output (in Kafka consumer terminal window):\n",
    "        {\"event_type\": \"purchase_health\", \"success\": true, \"Price\": 20, \"Accept\": \"*/*\", \"User-Agent\": \"ApacheBench/2.3\", \"Host\": \"user1.comcast.com\", \"health\": 10, \"id\": 1, \"size\": \"Medium Heart\"}\n",
    "        (etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use Spark to read in stream from Kafka, transform messages, create tables, and land them in HDFS\n",
    "    * Script: step4_write_hdfs_tables.sh\n",
    "    * Commands: \n",
    "        echo \"Launching write_hive_tables.py\"\n",
    "    docker-compose exec spark spark-submit /w205/w205_project3_Aguirre_Flowers_Mojekwu/pacomiguelagv/write_hive_tables.py\n",
    "    (see write_hive_tables.py for details)\n",
    "    * Output: \n",
    "     \n",
    "         21/04/11 03:07:54 INFO StreamExecution: Streaming query made progress: {\n",
    "  \"id\" : \"0107bd3c-002a-4b37-92b6-ceb5a12dd23c\",\n",
    "  \"runId\" : \"5a485a23-8398-4be7-b11a-ba281b0995f7\",\n",
    "  \"name\" : null,\n",
    "  \"timestamp\" : \"2021-04-11T03:07:54.000Z\",\n",
    "  \"numInputRows\" : 0,\n",
    "  \"inputRowsPerSecond\" : 0.0,\n",
    "  \"processedRowsPerSecond\" : 0.0,\n",
    "  \"durationMs\" : {\n",
    "    \"getOffset\" : 2,\n",
    "    \"triggerExecution\" : 2\n",
    "  },\n",
    "  \"stateOperators\" : [ ],\n",
    "  \"sources\" : [ {\n",
    "    \"description\" : \"KafkaSource[Subscribe[events]]\",\n",
    "    \"startOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 297\n",
    "      }\n",
    "    },\n",
    "    \"endOffset\" : {\n",
    "      \"events\" : {\n",
    "        \"0\" : 297\n",
    "      }\n",
    "    },\n",
    "    \"numInputRows\" : 0,\n",
    "    \"inputRowsPerSecond\" : 0.0,\n",
    "    \"processedRowsPerSecond\" : 0.0\n",
    "  } ],\n",
    "  \"sink\" : {\n",
    "    \"description\" : \"FileSink[/tmp/purchase_a_sword]\"\n",
    "  }\n",
    "}\n",
    "(etc)\n",
    "\n",
    "    * Check for presence of files: docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Transform parquet files into dataframes, send them to Hive\n",
    "    * Script: step5_make_hive_files.sh\n",
    "    * Commands: \n",
    "        echo \"Launching Hive_spark.py\"\n",
    "        watch -n 10 ./hdfs_to_hive_for_presto.sh\n",
    "    * Output: \n",
    "        Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "        21/04/11 03:08:18 INFO SparkContext: Running Spark version 2.2.0\n",
    "        21/04/11 03:08:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "        21/04/11 03:08:22 INFO SparkContext: Submitted application: ExtractEventsJob\n",
    "    (etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Query with presto\n",
    "\n",
    "    * Script: step6_launch_presto.py \n",
    "    * Command: docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "    * Output: NA, see queries below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examine a full table to see layout\n",
    "    * Query: select * from join_guild;\n",
    "    * Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " accept |       host        |   user-agent    | event_type | guild_name \n",
    "--------+-------------------+-----------------+------------+------------\n",
    " */*    | user3.att.com     | ApacheBench/2.3 | join_guild | Gnomes     \n",
    " */*    | user3.att.com     | ApacheBench/2.3 | join_guild | Gnomes        \n",
    " */*    | user3.att.com     | ApacheBench/2.3 | join_guild | Horde      \n",
    " */*    | user3.att.com     | ApacheBench/2.3 | join_guild | Horde   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count number of IDs present for each guild\n",
    "    * Query: select guild_name,  count(id) as num_ids from join_guild group by guild_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " guild_name | num_ids \n",
    "------------+---------\n",
    " Gods       |      24 \n",
    " Gnomes     |      27 \n",
    " Horde      |      25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Get average sword speed by purchase price\n",
    "     * Query: select price, avg(speed) as avg_speed from purchase_a_sword group by price;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " price | avg_speed \n",
    "-------+-----------\n",
    "   100 |       4.0 \n",
    "    50 |       1.0 \n",
    "    70 |       2.0 \n",
    "    20 |       5.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get average health by id\n",
    "    * select id, avg(health) as avg_health from purchase_health group by id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " id | avg_health \n",
    "----+------------\n",
    "  2 |       20.0 \n",
    "  1 |       10.0 \n",
    "  0 |        5.0 "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
